{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is written with the help of the github reference provided in the assignment as it waas allowed in the comment section of GC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from segment_anything import SamPredictor, SamAutomaticMaskGenerator\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "from gradio_demo.Matcher import Matcher\n",
    "from matcher.common import utils\n",
    "\n",
    "from dinov2.models import vision_transformer as vits\n",
    "import dinov2.utils.utils as dinov2_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"Images\"\n",
    "\n",
    "def mask_creator(img_size):\n",
    "    h, w = img_size\n",
    "    mask_h = int(h *0.3)\n",
    "    mask_w = int(w * 0.3)\n",
    "    \n",
    "    top = (h - mask_h) // 2\n",
    "    left = (w - mask_w) // 2\n",
    "    \n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "    mask[top:top+mask_h, left:left+mask_w] = 1\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(ref_path, target_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size=(518, 518)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    ref_img = Image.open(ref_path).convert('RGB')\n",
    "    target_img = Image.open(target_path).convert('RGB')\n",
    "    \n",
    "    ref_imgSize = (ref_img.size[1], ref_img.size[0])  \n",
    "    target_imgSize = (target_img.size[1], target_img.size[0]) \n",
    "    \n",
    "    ref_np = np.array(ref_img)\n",
    "    \n",
    "    mask = mask_creator(ref_np.shape[:2])\n",
    "    \n",
    "    ref_tensor = transform(ref_img)\n",
    "    target_tensor = transform(target_img)\n",
    "    \n",
    "    ref_mask_t = torch.from_numpy(mask)[None, None, ...].float()\n",
    "    ref_mask_t = F.interpolate(ref_mask_t, ref_tensor.size()[-2:], mode='nearest') > 0\n",
    "    \n",
    "    return {\n",
    "        \"support_img\": ref_tensor[None, ...],\n",
    "        \"support_mask\": ref_mask_t,\n",
    "        \"query_imgs\": target_tensor[None, ...],\n",
    "        \"support_img_ori_size\": ref_imgSize,\n",
    "        \"query_imgs_ori_size\": [target_imgSize],\n",
    "    }, ref_np, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n",
    "    model_type = \"default\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=device)\n",
    "\n",
    "    dinov2_kwargs = dict(\n",
    "        img_size=518,\n",
    "        patch_size=14,\n",
    "        init_values=1e-5,\n",
    "        ffn_layer='mlp',\n",
    "        block_chunks=0,\n",
    "        qkv_bias=True,\n",
    "        proj_bias=True,\n",
    "        ffn_bias=True,\n",
    "    )\n",
    "    dinov2 = vits.__dict__[\"vit_large\"](**dinov2_kwargs)\n",
    "    dinov2_utils.load_pretrained_weights(dinov2, \"models/dinov2_vitl14_pretrain.pth\", \"teacher\")\n",
    "    dinov2.eval()\n",
    "    dinov2.to(device=device)\n",
    "    \n",
    "    return sam, dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_process(data, predictor):\n",
    "    sup_mask = data['support_mask'].squeeze().cpu().numpy()\n",
    "    input_points = np.argwhere(sup_mask)  \n",
    "\n",
    "    center_y, center_x = np.mean(input_points, axis=0)\n",
    "    input_points = np.array([[center_x, center_y]])\n",
    "    \n",
    "    input_label = np.array([1] * len(input_points))\n",
    "    \n",
    "    support_img_np = data['support_img'].mul(255).byte()\n",
    "    support_img_np = support_img_np.squeeze().permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    predictor.reset_image()\n",
    "    predictor.set_image(support_img_np)\n",
    "    \n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True\n",
    "    )\n",
    "    predictor.reset_image()\n",
    "\n",
    "    best_mask_idx = np.argmax(scores)\n",
    "    data['support_mask'] = torch.tensor(masks[best_mask_idx:best_mask_idx+1])[None, ...]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matcher(dinov2, generator, device):\n",
    "    score_filter_cfg = {\n",
    "        \"emd\": 0.0,\n",
    "        \"purity\": 0.02,\n",
    "        \"coverage\": 0.0,\n",
    "        \"score_filter\": True,\n",
    "        \"score\": 0.33,\n",
    "        \"score_norm\": 0.1,\n",
    "        \"topk_scores_threshold\": 0.0\n",
    "    }\n",
    "    \n",
    "    matcher = Matcher(\n",
    "        encoder=dinov2,\n",
    "        generator=generator,\n",
    "        num_centers=8,\n",
    "        use_box=False,\n",
    "        use_points_or_centers=True,\n",
    "        sample_range=(1, 6),\n",
    "        max_sample_iterations=64,\n",
    "        alpha=1.0,\n",
    "        beta=0.0,\n",
    "        exp=0.0,\n",
    "        score_filter_cfg=score_filter_cfg,\n",
    "        num_merging_mask=9,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return matcher\n",
    "    \n",
    "def single_shot(ref_path, target_path, output_dir=\"results\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    sam, dinov2 = model_init(device)\n",
    "    \n",
    "    predictor = SamPredictor(sam)\n",
    "    generator = SamAutomaticMaskGenerator(\n",
    "        sam,\n",
    "        points_per_side=64,\n",
    "        points_per_batch=64,\n",
    "        pred_iou_thresh=0.88,\n",
    "        stability_score_thresh=0.95,\n",
    "        stability_score_offset=1.0,\n",
    "        sel_stability_score_thresh=0.90,\n",
    "        sel_pred_iou_thresh=0.85,\n",
    "        box_nms_thresh=0.65,\n",
    "        sel_output_layer=3,\n",
    "        output_layer=0,\n",
    "        dense_pred=True,\n",
    "        multimask_output=False,\n",
    "        sel_multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    matcher = create_matcher(dinov2, generator, device)\n",
    "\n",
    "    data, ref_np, initial_mask = load_images(ref_path, target_path)\n",
    "    data = mask_process(data, predictor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        utils.fix_randseed(0)\n",
    "        \n",
    "        support_imgs = data[\"support_img\"].to(device)[None, ...] \n",
    "        support_masks = data[\"support_mask\"].to(device) \n",
    "        matcher.set_reference(support_imgs, support_masks)\n",
    "        \n",
    "        query_img = data[\"query_imgs\"].to(device) \n",
    "        query_img_ori_size = data[\"query_imgs_ori_size\"][0]\n",
    "        matcher.set_target(query_img, query_img_ori_size)\n",
    "        \n",
    "        pred_mask, pred_mask_list = matcher.predict()\n",
    "        matcher.clear()\n",
    "        \n",
    "        support_img_ori_size = data['support_img_ori_size']\n",
    "        ref_mask = data['support_mask'].to(device).float()\n",
    "        ref_mask = F.interpolate(ref_mask, support_img_ori_size, mode=\"bilinear\", align_corners=False) > 0\n",
    "        ref_mask = ref_mask.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    target_img = Image.open(target_path).convert('RGB')\n",
    "    target_np = np.array(target_img)\n",
    "    \n",
    "    if isinstance(pred_mask, torch.Tensor):\n",
    "        pred_mask = pred_mask.cpu().numpy()\n",
    "    pred_mask = np.squeeze(pred_mask)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(ref_np)\n",
    "    axes[0, 0].set_title(\"Reference Image\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(ref_np)\n",
    "    red_initial = np.zeros((*initial_mask.shape, 4))  \n",
    "    red_initial[initial_mask > 0] = [1, 0, 0, 0.5]  \n",
    "    axes[0, 1].imshow(red_initial)\n",
    "    axes[0, 1].set_title(\"Initial Mask\")\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "\n",
    "    axes[1, 0].imshow(target_np)\n",
    "    axes[1, 0].set_title(\"Target Image\")\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "\n",
    "    axes[1, 1].imshow(target_np)\n",
    "    red_mask = np.zeros((*pred_mask.shape, 4))  \n",
    "    red_mask[pred_mask > 0] = [1, 0, 0, 0.7]  \n",
    "    axes[1, 1].imshow(red_mask)\n",
    "    axes[1, 1].set_title(\"Predicted Mask\")\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    base_name = os.path.basename(ref_path).split('.')[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_result.png\")\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " backpack: Images/backpack/00.jpg -> Images/backpack/05.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " backpack_dog: Images/backpack_dog/00.jpg -> Images/backpack_dog/01.jpg\n",
      " barn: Images/barn/00.jpg -> Images/barn/01.jpg\n",
      " bear_plushie: Images/bear_plushie/00.jpg -> Images/bear_plushie/01.jpg\n",
      " berry_bowl: Images/berry_bowl/00.jpg -> Images/berry_bowl/01.jpg\n",
      " can: Images/can/00.jpg -> Images/can/01.jpg\n",
      " candle: Images/candle/00.jpg -> Images/candle/01.jpg\n",
      " cat: Images/cat/00.jpg -> Images/cat/01.jpg\n",
      " cat2: Images/cat2/00.jpg -> Images/cat2/01.jpg\n",
      " cat_statue: Images/cat_statue/00.jpg -> Images/cat_statue/01.jpg\n",
      " chair: Images/chair/00.jpg -> Images/chair/01.jpg\n",
      " clock: Images/clock/00.jpg -> Images/clock/01.jpg\n",
      " colorful_sneaker: Images/colorful_sneaker/00.jpg -> Images/colorful_sneaker/01.jpg\n",
      " colorful_teapot: Images/colorful_teapot/00.jpg -> Images/colorful_teapot/01.jpg\n",
      " dog: Images/dog/00.jpg -> Images/dog/03.jpg\n",
      " dog2: Images/dog2/00.jpg -> Images/dog2/01.jpg\n",
      " dog3: Images/dog3/00.jpg -> Images/dog3/01.jpg\n"
     ]
    }
   ],
   "source": [
    "def image_straight(img_dir):\n",
    "    for subfolder in os.listdir(img_dir):\n",
    "        subfolder_path = os.path.join(img_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            image_files = [f for f in os.listdir(subfolder_path) if f.endswith(('.jpg'))]\n",
    "            ref_path = os.path.join(subfolder_path, image_files[0])\n",
    "            target_path = os.path.join(subfolder_path, image_files[1])\n",
    "            print(f\" {subfolder}: {ref_path} -> {target_path}\")\n",
    "            os.makedirs(f\"results/{subfolder}\", exist_ok=True)\n",
    "            single_shot(ref_path, target_path, output_dir=f\"results/{subfolder}\")\n",
    "\n",
    "\n",
    "image_straight(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing backpack: Images/backpack/05.jpg -> Images/backpack/00.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing backpack_dog: Images/backpack_dog/01.jpg -> Images/backpack_dog/00.jpg\n",
      "Processing barn: Images/barn/01.jpg -> Images/barn/00.jpg\n",
      "Processing bear_plushie: Images/bear_plushie/01.jpg -> Images/bear_plushie/00.jpg\n",
      "Processing berry_bowl: Images/berry_bowl/01.jpg -> Images/berry_bowl/00.jpg\n",
      "Processing can: Images/can/01.jpg -> Images/can/00.jpg\n",
      "Processing candle: Images/candle/01.jpg -> Images/candle/00.jpg\n",
      "Processing cat: Images/cat/01.jpg -> Images/cat/00.jpg\n",
      "Processing cat2: Images/cat2/01.jpg -> Images/cat2/00.jpg\n",
      "Processing cat_statue: Images/cat_statue/01.jpg -> Images/cat_statue/00.jpg\n",
      "Processing chair: Images/chair/01.jpg -> Images/chair/00.jpg\n",
      "Processing clock: Images/clock/01.jpg -> Images/clock/00.jpg\n",
      "Processing colorful_sneaker: Images/colorful_sneaker/01.jpg -> Images/colorful_sneaker/00.jpg\n",
      "Processing colorful_teapot: Images/colorful_teapot/01.jpg -> Images/colorful_teapot/00.jpg\n",
      "Processing dog: Images/dog/03.jpg -> Images/dog/00.jpg\n",
      "Processing dog2: Images/dog2/01.jpg -> Images/dog2/00.jpg\n",
      "Processing dog3: Images/dog3/01.jpg -> Images/dog3/00.jpg\n"
     ]
    }
   ],
   "source": [
    "def image_reverse(img_dir):\n",
    "    for subfolder in os.listdir(img_dir):\n",
    "        subfolder_path = os.path.join(img_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            image_files = [f for f in os.listdir(subfolder_path) if f.endswith(('.jpg'))]\n",
    "            ref_path = os.path.join(subfolder_path, image_files[0])\n",
    "            target_path = os.path.join(subfolder_path, image_files[1])\n",
    "            print(f\"Processing {subfolder}: {target_path} -> {ref_path}\")\n",
    "            os.makedirs(f\"results/{subfolder}\", exist_ok=True)\n",
    "            single_shot(target_path, ref_path, output_dir=f\"results/{subfolder}\")\n",
    "\n",
    "image_reverse(img_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
