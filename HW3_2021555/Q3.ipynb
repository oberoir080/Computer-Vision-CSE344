{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: open_clip_torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (2.2.1+cu121)\n",
      "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (0.17.1+cu121)\n",
      "Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open_clip_torch) (12.8.93)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision->open_clip_torch) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "from transformers import AutoProcessor\n",
    "from transformers import BlipForConditionalGeneration\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "blip = \"Salesforce/blip-image-captioning-base\"\n",
    "clip = \"openai/clip-vit-base-patch32\"\n",
    "clips = \"hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "\n",
    "for i in os.listdir(\"samples\"):\n",
    "    img_list.append(\"samples/\" + i)\n",
    "\n",
    "img_list = sorted(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "blip_proc = AutoProcessor.from_pretrained(blip)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(blip).to(device)\n",
    "\n",
    "\n",
    "for i in img_list:\n",
    "    img = Image.open(i)\n",
    "\n",
    "    inputs = blip_proc(images = img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = blip_model.generate(**inputs)\n",
    "    caption = blip_proc.batch_decode(ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    results.append({\n",
    "        \"image\": img,\n",
    "        \"blip caption\": caption,\n",
    "        \"clip score\": None,\n",
    "        \"clips score\": None\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_proc = CLIPProcessor.from_pretrained(clip)\n",
    "clip_model = CLIPModel.from_pretrained(clip).to(device)\n",
    "\n",
    "for i in results:\n",
    "    img = i[\"image\"]\n",
    "    caption = i[\"blip caption\"]\n",
    "\n",
    "    inputs = clip_proc(text=[caption], images=img, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        clip_score = logits_per_image.item()\n",
    "\n",
    "    i[\"clip score\"] = clip_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips_model, clips_preprocess = create_model_from_pretrained(clips)\n",
    "clips_tokenizer = get_tokenizer(clips)\n",
    "\n",
    "for i in results:\n",
    "    img = i[\"image\"]\n",
    "    caption = i[\"blip caption\"]\n",
    "    image = clips_preprocess(img).unsqueeze(0).to(device)\n",
    "    text = clips_tokenizer([caption]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clips_model.encode_image(image)\n",
    "        text_features = clips_model.encode_text(text)\n",
    "\n",
    "        image_features_norm = F.normalize(image_features, dim=-1)\n",
    "        text_features_norm = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        cosine_similarity = (image_features_norm @ text_features_norm.T).item()\n",
    "\n",
    "        logit_scale = clips_model.logit_scale.exp().item()\n",
    "        scaled_similarity = cosine_similarity * logit_scale\n",
    "\n",
    "        i[\"clips score\"] = scaled_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x302>,\n",
       "  'blip caption': 'a small dog walking on a green carpet',\n",
       "  'clip score': 31.566017150878906,\n",
       "  'clips score': 24.999535889562594},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x366>,\n",
       "  'blip caption': 'a small dog running across a green field',\n",
       "  'clip score': 32.70260238647461,\n",
       "  'clips score': 26.338922534023368},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x300>,\n",
       "  'blip caption': 'a family sitting in a pool with a towel',\n",
       "  'clip score': 31.336462020874023,\n",
       "  'clips score': 20.4160130104367},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x320>,\n",
       "  'blip caption': 'a small bird sitting on a plant',\n",
       "  'clip score': 28.938268661499023,\n",
       "  'clips score': 23.344033408776568},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x300>,\n",
       "  'blip caption': 'a small dog standing on a stone ledge',\n",
       "  'clip score': 31.03472900390625,\n",
       "  'clips score': 20.79832140759322},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x300>,\n",
       "  'blip caption': 'a man riding a bike down a wet street',\n",
       "  'clip score': 30.83451271057129,\n",
       "  'clips score': 23.270816916710828},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x300>,\n",
       "  'blip caption': 'a brown butterfly sitting on a green plant',\n",
       "  'clip score': 28.916311264038086,\n",
       "  'clips score': 23.02419456105895},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=323x400>,\n",
       "  'blip caption': 'a man in a suit and tie sitting on a couch',\n",
       "  'clip score': 28.895353317260742,\n",
       "  'clips score': 17.54190968517264},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x396>,\n",
       "  'blip caption': 'a duck drinking water from a pond',\n",
       "  'clip score': 30.52521514892578,\n",
       "  'clips score': 22.711961557198492},\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=342x400>,\n",
       "  'blip caption': 'a coffee machine with two cups on it',\n",
       "  'clip score': 27.961315155029297,\n",
       "  'clips score': 21.45969993608105}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
